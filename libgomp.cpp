// libgomp.cpp
//
// A subset of libgomp for bare metal.
// This is a simplistic implementation for a single-core processor for learning, development, and testing.
//
// Target is an ARM CPU with very simple bare metal threading support.
//
// These implementations only work on a single-core,non-preemtive system.


#include <stdint.h>
#include <omp.h>
#include "thread.hpp"
#include "threadFIFO.hpp"
#include "libgomp.hpp"

// The worker thread's stacks
// The thread number can be determined by  looking at certain bits of the sp.
static char gomp_stacks[GOMP_NUM_THREADS][GOMP_STACK_SIZE] __attribute__((__aligned__((4096))));    // TODO align to 4K for debug, later make this GOMP_STACK_SIZE

typedef void WORKFN(void *);


struct worker
    {
    Thread thread;          // the thread
    WORKFN *fn;             // the thread function generated by OMP
    void *data;             // the thread's local data pointer
    int team = 0;           // the current team number, nonzero if running
    unsigned single = 0;    // counts the number of "#pragma omp single" seen
    bool arrived = false;   // arrived at a barrier, waiting for other threads to arrive
    bool mwaiting = false;  // waiting on a mutex
    };

// an array of workers
static worker workers[GOMP_NUM_THREADS];

// These arrays indexed by "team" should be converted to a pool of "team" structs.
// I don't think teams are necessarily nested in scope.
static bool mutex[GOMP_NUM_TEAMS] = {0};
static int mindex = 0;
static unsigned single[GOMP_NUM_TEAMS] = {0};
static int sections_count[GOMP_NUM_TEAMS] = {0};
static int sections[GOMP_NUM_TEAMS] = {0};
static int section[GOMP_NUM_TEAMS] = {0};


int dyn_var = 0;

static int gomp_num_threads = GOMP_NUM_THREADS;
static int team = 0;



// this is the code for every member of the thread pool 

static void gomp_worker()
    {
    int id = omp_get_thread_num();

    while(true)
        {
        workers[id].team = 0;                       // zero the team to indicate the member has finished
        workers[id].thread.suspend();               //  wait for an new assignnment

        WORKFN *fn = workers[id].fn;                // run the assigned code
        void *data = workers[id].data;
        (*fn)(data);
        }
    }



// Powerup initialization of libgomp.
// Must be called after thread and threadFIFO are setup.
// It spawns a pool of OMP worker threads.

void libgomp_init()
    {
    for(int i=0; i<GOMP_NUM_THREADS; i++)
        {
        Thread::spawn(gomp_worker, gomp_stacks[i]);         
        }
    }



extern "C"
void GOMP_parallel(
    WORKFN *fn,                                     // the thread code
    void *data,                                     // the team local data
    unsigned num_threads,                           // the requested number of threads
    unsigned flags __attribute__((__unused__)))     // flags (ignored for now)
    {

    // TODO fix this to handle teams of less than max threads
    // starting at other than i=0

    if(num_threads == 0)
        {
        num_threads = gomp_num_threads;
        }

    if(num_threads > GOMP_NUM_THREADS)
        {
        num_threads = GOMP_NUM_THREADS;
        }

    ++team;

    mutex[team] = false;
    single[team] = 0;
    sections_count[team] = 0;
    sections[team] = 0;
    section[team] = 0;

    // setup each member of the team
    for(unsigned i=0; i<num_threads; i++)
        {
        workers[i].fn = fn;
        workers[i].data = data;
        workers[i].arrived = false;
        workers[i].mwaiting = false;
        workers[i].single = 0;
        workers[i].team = team;
        }

    // start each member of the team
    for(unsigned i=0; i<num_threads; i++)
        {
        workers[i].thread.resume();
        }
    
    // wait for each team member to complete
    for(unsigned i=0; i<num_threads; i++)
        {
        while(workers[i].team == team)
            {
            yield();
            }
        }

    --team;
    }


extern "C"
void GOMP_barrier()
    {
    int id = omp_get_thread_num();
    int num = omp_get_num_threads();

    workers[id].arrived = true;                 // signal that this thread has reached the barrier

    for(int i=0; i<num; i++)                    // see if any other threads in the team have not yet reached the barrier
        {
        if(workers[i].team == team
        && workers[i].arrived == false)
            {                                   // get here if any team member has not yet arrived
            workers[id].thread.suspend();       // suspend this thread until all other threads have arrived
            return;                             // when resumed, some other thread has done all the barrier cleanup work, so just keep going
            }
        }

    // get here if all threads in the team have arrived at the barrier

    for(int i=0; i<num; i++)                    // clear the arrived flag for all team members
        {
        if(workers[i].team == team)
            {
            workers[i].arrived = false;         // clear the arrived-at-barrier flag
            }
        }

    for(int i=0; i<num; i++)                    // resume all other threads in the team
        {
        if(i != id                              // don't try to resume self
        && workers[i].team == team)
            {
            workers[i].thread.resume();         // resume the team member
            }
        }
    }


extern "C"
void GOMP_critical_start()
    {
    int id = omp_get_thread_num();

    while(mutex[team] == true)
        {
        workers[id].mwaiting = true;
        workers[id].thread.suspend();              // suspend this thread until it can grab the mutex
        workers[id].mwaiting = false;
        }

    mutex[team] = true;
    }

extern "C"
void GOMP_atomic_start()
    {
    GOMP_critical_start();
    }


extern "C"
void GOMP_critical_end()
    {
    int num = omp_get_num_threads();

    mutex[team] = false;

    for(int i=0; i<num; i++)                    // resume all other threads in the team
        {
        int m = mindex;
        mindex = (mindex+1)%num;

        if(workers[m].team == team
        && workers[m].mwaiting == true)
            {
            workers[m].thread.resume();         // resume the next thread in the rotation
            return;
            }
        }
    }

extern "C"
void GOMP_atomic_end()
    {
    GOMP_critical_end();
    }



extern "C"
bool GOMP_single_start()
    {
    int id = omp_get_thread_num();

    if(workers[id].single++ == single[team])
        {
        single[team]++;
        return true;
        }
    else
        {
        return false;
        }
    }





extern "C"
int GOMP_sections_next()                // for each thread that iterates the "sections"
    {
    if(section[team] > sections[team])  // if all sections have been executed
        {
        section[team] = 0;              // clear the index, it latches at zero
        }

    if(section[team] == 0)              // if all sections have been run
        {
        return 0;                       // return 0, select the "end" action and stop iterating
        }

    return section[team]++;             // otherwise return the current index and increment it
    }

extern "C"
int GOMP_sections_start(int num)        // each team member calls this once at the beginning of sections
    {
    if(sections_count[team] == 0)       // when the first thread gets here
        {
        sections[team] = num;           // capture the number of sections
        section[team] = 1;              // init to the first section
        }

    ++sections_count[team];             // count the number of threads that have started the sections

    return GOMP_sections_next();        // for the rest, start is the same as next
    }

extern "C"
void GOMP_sections_end()                // each thread runs this once when all the sections hae been executed
    {
    int num = omp_get_num_threads();

    if(sections_count[team] == num)     // if all team members have encountered the "start"
        {
        sections_count[team] = 0;       // re-arm the sections start, though note that some may still be in a section
        }

    GOMP_barrier();                     // hold everyone here until all have arrived
    }


/////////////
// LOCKING //
/////////////

extern "C"
void omp_init_lock(omp_lock_t *lock)
    {
    *lock = 0;
    }

extern "C"
void omp_set_lock(omp_lock_t *lock)
    {
    while(*lock)
        {
        yield();
        }

    *lock = 1;
    }

extern "C"
void omp_unset_lock(omp_lock_t *lock)
    {
    *lock = 0;
    }

extern "C"
int omp_test_lock(omp_lock_t *lock)
    {
    if(*lock)
        {
        return 0;
        }

    *lock = 1;
    return 1;
    }

extern "C"
void omp_destroy_lock(omp_lock_t *lock)
    {
    *lock = 0;
    }



////////////////////
// NESTED LOCKING //
////////////////////

extern "C"
void omp_init_nest_lock(omp_nest_lock_t *lock)
    {
    lock->lock = 0;
    lock->count = 0;
    lock->owner = 0;
    }

extern "C"
void omp_set_nest_lock(omp_nest_lock_t *lock)
    {
    int id = omp_get_thread_num();

    if(lock->lock && lock->owner == id)
        {
        ++lock->count;
        return;
        }
 
    while(lock->lock==1)
        {
        yield();
        }

    lock->lock = 1;
    lock->count = 1;
    lock->owner = id;
    }

extern "C"
void omp_unset_nest_lock(omp_nest_lock_t *lock)
    {
    // It is assumed that the caller matches the owner. This is not checked.
    // It is assumed that count>0. This is not checked.

    --lock->count;
    if(lock->count == 0)
        {
        lock->owner = 0;
        lock->lock = 0;
        }
    }

extern "C"
int omp_test_nest_lock(omp_nest_lock_t *lock)
    {
    int id = omp_get_thread_num();

    if(lock->lock && lock->owner == id)
        {
        ++lock->count;
        return lock->count;
        }
 
    if(lock->lock == 0)
        {
        lock->lock = 1;
        lock->count = 1;
        lock->owner = id;
        return 1;
        }

    return 0;
    }

extern "C"
void omp_destroy_nest_lock(omp_nest_lock_t *lock)
    {
    lock->lock = 0;
    lock->count = 0;
    lock->owner = 0;
    }



///////////
// Tasks //
///////////

extern "C"
void GOMP_task (    void (*fn) (void *),
                    void *data,
                    void (*cpyfn) (void *, void *),
                    long arg_size,
                    long arg_align,
                    bool if_clause,
                    unsigned flags,
                    void **depend,
                    int priority_arg,
                    void *detach)
    {
    if(!if_clause)                              // if if_clause if false we have to run the task right no
        {
        if(cpyfn)                               // if a copy fucntion is defined, copy the data to a private buffer first
            {
            char buf[arg_size + arg_align - 1];
            char *dst = &buf[arg_align-1];
            dst = (char *)((uintptr_t)dst & (arg_align-1));
            cpyfn(dst, data);
            fn(dst);            
            }
        else
            {
            fn(data);
            }
        }
    else
        {
        
        }

    }



/////////////////////////////////
// Explicitly called functions //
/////////////////////////////////

// return the number of threads available
extern "C"
int omp_get_num_threads()
    {
    return GOMP_NUM_THREADS;
    }


extern "C"
int omp_get_thread_num()
    {
    uintptr_t sp;
    uintptr_t base = (uintptr_t)&gomp_stacks;

    __asm__ __volatile__("    mov %[sp], sp" : [sp]"=r"(sp));

    return (sp-base)/GOMP_STACK_SIZE;
    }


// extern "C" void omp_set_num_threads (int);
// extern "C" int omp_get_max_threads (void);



extern "C"
void omp_set_dynamic(int dyn)
    {
    dyn_var = dyn;
    }

extern "C"
int omp_get_dynamic(void)
    {
    return dyn_var;
    }



// extern "C" int omp_get_num_teams (void);
// extern "C" int omp_get_team_num (void);
// extern "C" int omp_get_team_size (int);


// extern "C" int omp_get_num_procs (void);
// extern "C" int omp_in_parallel (void);
// extern "C" void omp_set_nested (int);
// extern "C" int omp_get_nested (void);
// extern "C" void omp_init_lock_with_hint (omp_lock_t *, omp_sync_hint_t);
// extern "C" void omp_init_nest_lock_with_hint (omp_nest_lock_t *, omp_sync_hint_t);
// extern "C" double omp_get_wtime (void);
// extern "C" double omp_get_wtick (void);
// extern "C" void omp_set_schedule (omp_sched_t, int);
// extern "C" void omp_get_schedule (omp_sched_t *, int *);
// extern "C" int omp_get_thread_limit (void);
// extern "C" void omp_set_max_active_levels (int);
// extern "C" int omp_get_max_active_levels (void);
// extern "C" int omp_get_level (void);
// extern "C" int omp_get_ancestor_thread_num (int);
// extern "C" int omp_get_active_level (void);
// extern "C" int omp_in_final (void);
// extern "C" int omp_get_cancellation (void);
// extern "C" omp_proc_bind_t omp_get_proc_bind (void);
// extern "C" int omp_get_num_places (void);
// extern "C" int omp_get_place_num_procs (int);
// extern "C" void omp_get_place_proc_ids (int, int *);
// extern "C" int omp_get_place_num (void);
// extern "C" int omp_get_partition_num_places (void);
// extern "C" void omp_get_partition_place_nums (int *);
// extern "C" void omp_set_default_device (int);
// extern "C" int omp_get_default_device (void);
// extern "C" int omp_get_num_devices (void);
// extern "C" int omp_is_initial_device (void);
// extern "C" int omp_get_initial_device (void);
// extern "C" int omp_get_max_task_priority (void);
// extern "C" void *omp_target_alloc (__SIZE_TYPE__, int);
// extern "C" void omp_target_free (void *, int);
// extern "C" int omp_target_is_present (const void *, int);
// extern "C" int omp_target_memcpy (void *, const void *, __SIZE_TYPE__, __SIZE_TYPE__, __SIZE_TYPE__, int, int);
// extern "C" int omp_target_memcpy_rect (void *, const void *, __SIZE_TYPE__, int, const __SIZE_TYPE__ *, const __SIZE_TYPE__ *, const __SIZE_TYPE__ *, const __SIZE_TYPE__ *, const __SIZE_TYPE__ *, int, int);
// extern "C" int omp_target_associate_ptr (const void *, const void *, __SIZE_TYPE__, __SIZE_TYPE__, int);
// extern "C" int omp_target_disassociate_ptr (const void *, int);
// extern "C" void omp_set_affinity_format (const char *);
// extern "C" __SIZE_TYPE__ omp_get_affinity_format (char *, __SIZE_TYPE__);
// extern "C" void omp_display_affinity (const char *);
// extern "C" __SIZE_TYPE__ omp_capture_affinity (char *, __SIZE_TYPE__, const char *);
// extern "C" int omp_pause_resource (omp_pause_resource_t, int);
// extern "C" int omp_pause_resource_all (omp_pause_resource_t);
